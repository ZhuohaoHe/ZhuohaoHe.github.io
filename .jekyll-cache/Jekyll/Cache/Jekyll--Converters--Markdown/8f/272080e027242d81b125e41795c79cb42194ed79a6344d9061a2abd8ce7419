I"ã&<h2 id="åˆ†ç±»logisticå›å½’">åˆ†ç±»ï¼šLogisticå›å½’</h2>

<p>ä¸Šä¸€ç¯‡æ–‡ç« æˆ‘ä»¬ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­çš„å›å½’æ–¹æ³•ï¼Œé‚£èƒ½å¦å°†å›å½’æ–¹æ³•ç›´æ¥åº”ç”¨äºåˆ†ç±»é—®é¢˜å‘¢ï¼Ÿ</p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/regression-bad-result.jpg" alt="bad result" /></p>

<p><em>è“è‰²ç›´çº¿</em> : åœ¨å‰å…«ä¸ªæ•°æ®ä¹‹å‰ï¼Œåˆ©ç”¨å›å½’æ¥åˆ†ç±»ï¼Œå¾—åˆ°çš„åˆ†ç±»ç»“æœè¿˜æ˜¯ç›¸å¯¹å‡†ç¡®çš„ã€‚</p>

<p><em>çº¢è‰²ç›´çº¿</em> : ä½†æ˜¯å½“ç¬¬ä¹ä¸ªæ•°æ®å‡ºç°ä¹‹åï¼Œå›å½’ç›´çº¿ç”±äºè¿™ä¸€ä¸ªæ•°æ®ï¼Œäº§ç”Ÿäº†åç§»ï¼Œå‡ºç° $ h(x) &gt; 1 $ æˆ–è€… $ h(x) &lt; 0 $ çš„æƒ…å†µï¼Œå¾ˆæ˜¾ç„¶åˆ†ç±»é”™è¯¯ã€‚</p>

<p><em>ç»¿è‰²ç›´çº¿</em> : æ­£ç¡®çš„åˆ†ç±»ç»“æœ</p>

<p>ä¸ºäº†è§£å†³ <em>çº¢è‰²ç›´çº¿</em> çš„é”™è¯¯åˆ†ç±»ï¼Œå‡ºç°äº† <strong>Logisticå›å½’</strong></p>

<h4 id="logistic-regression">Logistic Regression</h4>

<p>ä½¿ç”¨ <strong>Logisticå›å½’</strong> çš„ç›®çš„æ˜¯æ§åˆ¶ $ 0 \leq h(x) \leq 1 $ ï¼ŒåŸæœ¬ä½¿ç”¨çš„ $ h_\theta(x) = \theta^Tx $ ä¸ç¬¦åˆè¦æ±‚ï¼Œæ‰€ä»¥éœ€è¦å¯¹ $ h_\theta(x) $ æ”¹è¿›ï¼Œ</p>

\[h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}\]

<p>å…¶ä¸­ï¼Œg(z)å‡½æ•°è¢«ç§°ä¸º logistic å‡½æ•° æˆ– sigmoid å‡½æ•°</p>

\[g(z) = \frac{1}{1+e^{-z}}\]

<p><strong>sigmoid å‡½æ•°çš„æ€§è´¨</strong></p>

\[g'(z) = g(z)(1-g(z))\]

<p><strong>æ¦‚ç‡è§£é‡Š</strong></p>

<p>$h_\theta(x)$:å¯¹äºè¾“å…¥ x ï¼Œ è¾“å‡º y = 1 çš„å¯èƒ½æ€§</p>

\[P(y=0|x;\theta) + P(y=0|x;\theta) = 1\]

<p><strong>åˆ†ç±»è¾¹ç•Œ</strong></p>

<p>ç”±äºå‡½æ•° $ 0 \leq h_\theta(x) \leq 1 $ ,æ‰€ä»¥ï¼Œ</p>

\[y = \begin{cases}
	1, if \space h_\theta(x) &gt; 0.5\\
	0, if \space h_\theta(x) &lt; 0.5
\end{cases}\]

<p>åˆæœ‰</p>

\[h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^ Tx}}\]

<p>æ ¹æ® sigmoid å‡½æ•°çš„æ›²çº¿å›¾åƒï¼Œ</p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/sigmoid.jpg" alt="sigmoid" style="zoom: 33%;" /></p>

<p>å¯ä»¥å¾—åˆ°åˆ†ç±»è¾¹ç•Œï¼Œ</p>

\[\begin{cases}
	\theta^Tx &gt; 0, if \space y = 1\\
	\theta^Tx &lt; 0, if \space y = 0
\end{cases}\]

<p>ä¾‹å¦‚ä¸‹å›¾ï¼Œ$ h_\theta(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2) $</p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/edge1.jpg" alt="edge1" style="zoom: 25%;" /></p>

<p>å¦‚æœ $ -3+x_1+x_2 \geq 0 $ , åˆ™é¢„æµ‹ â€œy = 1â€</p>

<p>æˆ–å¯¹äºéçº¿æ€§åˆ†ç±»è¾¹ç•Œï¼Œ$ h_\theta(X) = g(\theta_0+ \theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_3^2) $</p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/edge2.jpg" alt="edge2" style="zoom:25%;" /></p>

<p>å¦‚æœ $ -1+x_1^2+x_2^2\geq0 $, åˆ™é¢„æµ‹ â€œy = 1â€</p>

<h4 id="è¾“å…¥">è¾“å…¥</h4>

<ul>
  <li>
    <p>æ¯ä¸ªæ ·æœ¬ä¸­çš„ x éƒ½æœ‰ç›¸åº”çš„ y å¯¹åº”ï¼Œå³æ ·æœ¬é›†</p>

\[\lbrace (x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})  \rbrace\]
  </li>
  <li>
    <p>x æœ‰ m ä¸ªæ ·æœ¬ï¼Œ n ç»´ç‰¹å¾</p>
  </li>
</ul>

\[X_{m \times (n+1)} = \left[
	\begin{matrix}
	\vec x^{(1)}\\
	...\\
	\vec x^{(n)}
	\end{matrix}
\right], å…¶ä¸­ \vec x^{(j)} = (1, x_1^{(j)}, ..., x_n^{(j)})\]

<ul>
  <li>y æœ‰</li>
</ul>

\[y_{m\times1} = \left[\begin{matrix}
	y^{(1)}\\
	...\\
	y^{(m)}
\end{matrix}\right]\]

<h4 id="æŸå¤±å‡½æ•°">æŸå¤±å‡½æ•°</h4>

<blockquote>
  <p>èƒ½å¦ç”¨çº¿æ€§å›å½’ä¸­çš„å¹³æ–¹æŸå¤±å‡½æ•°ï¼Ÿ</p>

  <p>ä¸èƒ½ï¼Œå› ä¸ºå¯¹äº $ h_\theta(x) $ æ¥è¯´ï¼Œå¹³æ–¹æŸå¤±å‡½æ•°æ˜¯éå‡¸å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°çš„æœ€ä¼˜é€‰æ‹©æ˜¯å‡¸å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç»§ç»­å¯»æ‰¾æ›´å¥½çš„æŸå¤±å‡½æ•°ã€‚</p>

  <p>ä»€ä¹ˆæ˜¯å‡¸å‡½æ•°ï¼Ÿ
<img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/convex-function.jpg" alt="convex function" style="zoom: 35%;" /></p>
</blockquote>

<p>0-1æŸå¤±å‡½æ•°ï¼Ÿ</p>

\[l(h_\theta(x), y) = \begin{cases} 
	1, if {\space} h_\theta(x) \ne y\\
	0, otherwise
\end {cases}\]

<p>0-1æŸå¤±å‡½æ•°æ˜¯æœ€èƒ½å‡†ç¡®è¡¨è¾¾æ¦‚ç‡çš„å‡½æ•°ï¼Œä½†æ˜¯ç”±äºå®ƒæ˜¯ç¦»æ•£çš„å‡½æ•°ï¼Œä¸èƒ½å¤Ÿè¿›è¡Œä¼˜åŒ–ï¼Œæ‰€ä»¥é€‰æ‹©0-1æŸå¤±å‡½æ•°å¹¶ä¸åˆé€‚ã€‚</p>

<blockquote>
  <p>ä¼˜åŒ–ï¼šé€šè¿‡å¯¹å‚æ•°çš„è°ƒæ•´ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æ”¶æ•›åˆ°æœ€å°å€¼ï¼Œä¾‹å¦‚ä¸Šä¸€èŠ‚çš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œä½†æ˜¯ç”±äºå‡½æ•°æ˜¯ç¦»æ•£çš„ï¼Œå¯¼æ•°ç­‰å·¥å…·éƒ½æ— æ³•ä½¿ç”¨ï¼Œæ‰€ä»¥ä¸èƒ½è¿›è¡Œä¼˜åŒ–ã€‚</p>
</blockquote>

<p>ç”¨å…¶ä»–æŸå¤±å‡½æ•°æ›¿ä»£0-1æŸå¤±å‡½æ•°ï¼Ÿ</p>

<p>æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼</p>

\[P(y=1|x;\theta) = h_\theta(x) \\
P(y=0|x;\theta) = 1-h_\theta(x)\]

\[p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}\]

<p>æŸå¤±å‡½æ•°ï¼Œ</p>

\[L(\theta) = p(y|X;\theta) 
=\prod^m_{i=1} p(y^{(i)}|x^{(i)};\theta)\\
=\prod^m_{i=1} (h_\theta(x))^{y^{(i)}}(1-h_\theta(x))^{1-y^{(i)}}\]

<p><strong>LogisticæŸå¤±å‡½æ•°</strong></p>

\[l(\theta) = -logL(\theta) \\
=-\left[ \sum^m_{i=1}y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})) \right]\]

\[Cost(h_\theta(x^{(i)}), y) = \begin{cases}
	-log(h_\theta(x)), \space if \space y = 1 \\
	-log(1-h_\theta(x)), \space if \space y = 0
\end{cases}\]

<p>é€šè¿‡å›¾åƒæ¥éªŒè¯æŸå¤±å‡½æ•°çš„ç»“æœï¼Œ</p>

<p><em>å½“ y = 1 æ—¶ï¼Œ éšç€ $ h_\theta(x) $ é€æ¸æ¥è¿‘1ï¼ŒæŸå¤±å‡½æ•°é€æ¸å‡å°</em></p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/loss1.jpg" alt="loss1" style="zoom:30%;" /></p>

<p><em>å½“ y = 0 æ—¶ï¼Œ éšç€ $ h_\theta(x) $ é€æ¸æ¥è¿‘1ï¼ŒæŸå¤±å‡½æ•°é€æ¸å¢å¤§</em></p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/loss2.jpg" alt="loss2" style="zoom:33%;" /></p>

<h4 id="æ¢¯åº¦ä¸‹é™">æ¢¯åº¦ä¸‹é™</h4>

\[J(\theta) =-\left[ \sum^m_{i=1}y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})) \right]\]

<p>æ¢¯åº¦ä¸‹é™æ¨¡æ¿</p>

<p><code class="language-plaintext highlighter-rouge">repate {</code></p>

\[\theta_j := \theta_j-\alpha \frac{\partial}{\partial\theta_j}J(\theta)\]

<p><code class="language-plaintext highlighter-rouge">}</code></p>

<p>å¯¹äº logistic å›å½’çš„ $ J(\theta) $ ,æ±‚ å¯¼æ•°å¾—ï¼Œ</p>

\[\frac{\partial}{\partial\theta_j}J(\theta) = (h_\theta(x)-y)x_j\]

<p><em>æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹å›¾</em></p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/derivative.jpg" alt="derivative" style="zoom: 33%;" /></p>

<blockquote>
  <p>ä¼¼ä¹å’Œä¸Šä¸€èŠ‚çš„çº¿æ€§å›å½’ç›¸åŒï¼Ÿæ˜¯å¦çœŸçš„ç›¸åŒå‘¢ï¼Ÿ</p>

  <p>ï¼Ÿï¼Ÿï¼Ÿ</p>
</blockquote>

<p>æ‰€ä»¥æ¢¯åº¦ä¸‹é™å…¬å¼å¯ä»¥æ›´æ–°ä¸ºï¼Œ</p>

<p><code class="language-plaintext highlighter-rouge">repate {</code></p>

\[\theta_j := \theta_j-\alpha (h_\theta(x)-y)x_j\]

<p><code class="language-plaintext highlighter-rouge">}</code></p>

<p>(æ³¨æ„ï¼Œ$ \theta $ éœ€è¦åŒæ­¥æ›´æ–°ï¼Œå³æ¯æ¬¡éƒ½å°†æ‰€æœ‰çš„ $ \theta $ æ›´æ–°)</p>

<blockquote>
  <p>åˆ©ç”¨æ¢¯åº¦ä¸‹é™éªŒè¯ï¼šæ˜¯å¦èƒ½ç”¨çº¿æ€§å›å½’ä¸­çš„å¹³æ–¹æŸå¤±ï¼Ÿ</p>

  <p>æœ‰ï¼Œ</p>

\[\begin{cases} 
J(\theta) = \frac1{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
\\
h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}\\
\\
g'(z) = g(z)(1-g(z))
\end{cases}\]

\[\frac{\partial}{\partial\theta_j}J(\theta)=(g(\theta^Tx)-y)g(\theta^Tx)(1-g(\theta^Tx))x_j\]

  <p>æ¨å¯¼å¦‚ä¸‹å›¾ï¼Œ</p>

  <p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/derivative.jpg" alt="derivative2" style="zoom:33%;" /></p>

  <p>å‡è®¾ â€œy = 0â€,</p>

\[if\space h_\theta(x) = 1\space {} \space(far\space form\space target) \rightarrow \frac{\partial}{\partial\theta_j}J(\theta) = 0 \\
if\space h_\theta(x) = 0\space {} \space(close\space to\space target) \rightarrow \frac{\partial}{\partial\theta_j}J(\theta) = 0\]

  <p>å¯ä»¥å‘ç°ï¼Œå½“é¢„æµ‹ç»“æœæ¥è¿‘æˆ–è€…åŸç†çœŸå®å€¼æ—¶ï¼Œæ¢¯åº¦éƒ½ä¸º0ï¼Œå³å¹¶ä¸èƒ½æ€»æ˜¯å¾—åˆ°ä¼˜è§£ã€‚</p>

  <p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/square-error.jpg" alt="Square Error" style="zoom:50%;" /></p>
</blockquote>

<h4 id="softmax-regression">Softmax Regression</h4>

<p>åœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬é‡åˆ°çš„é—®é¢˜ï¼Œå¾€å¾€éƒ½ä¸æ˜¯ç®€å•çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œè€Œæ˜¯æ›´ä¸ºå¤æ‚çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œè€Œæˆ‘ä»¬ç›®å‰å­¦åˆ°çš„çŸ¥è¯†ï¼Œæ²¡æœ‰åŠæ³•ç›´æ¥è§£å†³ä¸€ä¸ªå¤šåˆ†ç±»é—®é¢˜ã€‚</p>

<p>è™½ç„¶æ— æ³•ç›´æ¥å¤šåˆ†ç±»ï¼Œä½†æ˜¯å¯ä»¥å°†å¤šåˆ†ç±»è½¬æ¢æˆå¤šä¸ªäºŒåˆ†ç±»ï¼Œå³è½¬æ¢æˆå¤šä¸ªä¸€å¯¹å¤šçš„åˆ†ç±»ï¼Œå¦‚ä¸‹å›¾ã€‚</p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/one2many.jpg" alt="one2many" style="zoom: 40%;" /></p>

<p>ä¸ºæ¯ç±»è®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨ $ h_{\theta^i}(x) $ ç”¨æ¥é¢„æµ‹ $ y = i $ çš„å¯èƒ½æ€§.
å¯¹äºä¸€ä¸ªæ–°è¾“å…¥ x , åšä¸€ä¸ªé¢„æµ‹, é€‰æ‹©ä¸€ä¸ªç±»åˆ« i , ä½¿å¾—ï¼š</p>

\[\max_ih_{\theta^i}(x)\]

<p>ä½†æ˜¯ç”±äº $ \sum h_{\theta^i}(x) \ne 1 $ ï¼Œæ‰€ä»¥æ¦‚ç‡ä¹‹å’Œä¸ä¸º1ï¼Œå¹¶ä¸èƒ½å¾ˆæ˜ç¡®çš„å¯¹æ¦‚ç‡è¿›è¡Œæ¯”è¾ƒï¼Œæ‰€ä»¥éœ€è¦å¼•å…¥ <strong>Softmax Regression</strong></p>

\[p(y=i|x;\theta) = h_{\theta^i}(x) = \frac{e^{(\theta^i)^T}x}{\sum_{j=1}^K{e^{(\theta^i)^T}x}}\]

<p>ä»¤ $ z = (\theta^i)^Tx $ å¾—ï¼Œ</p>

\[p(y=i|x;\theta) = h_{\theta^i}(x) = \frac {e^z} {\sum_{j=1}^K{e^z}}\]

<p>ç”¨å¯¹æ•°ä¼¼ç„¶ä¼°è®¡å¾—ï¼Œ</p>

\[L(\theta) = \sum^m_{i=1}log(p(y=i|x;\theta))\\
=\sum^m_{i=1}log(\frac {e^{z_{y^{(i)}}}} {\sum_{j=1}^K{e^{z_{y^{(i)}}}}})\]

<p>æŸå¤±å‡½æ•°ä¸ºï¼Œ</p>

\[loss(\theta) = -L(\theta) = -\sum^m_{i=1}log(\frac {e^{z_{y^{(i)}}}} {\sum_{j=1}^K{e^{z_{y^{(i)}}}}})\\
=\sum^m_{i=1}\left[ log(\sum_{j=1}^K{e^{z_j}})-z_{y^{(i)}} \right]\]

<p>æ‰€ä»¥ï¼Œå•ä¸ªæ ·æœ¬ i å¯¹åº”å¾—æŸå¤±ä¸ºï¼Œ</p>

\[loss_i(\theta)=log(\sum_{j=1}^K{e^{z_j}})-z_{y^{(i)}}\]

<blockquote>
  <p>$ loss_i $ çš„å–å€¼èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ</p>

  <p>å› ä¸º $ loss_i(\theta) = -log(h_{\theta^i}(x)) $</p>

  <p>è€Œ $ 0 \leq h_{\theta^i}(x) \leq 1 $, æ‰€ä»¥ $ loss_i \geq 0$</p>

  <p>åˆå§‹åŒ–æ¯ç±»çš„å‚æ•° $ \theta^l \approx 0 $, $ loss_i = ? $</p>

  <p>? ? ?</p>
</blockquote>

<p><strong>Softmax Regression æµç¨‹</strong></p>

<p><img src="/img/in-post/2020-10-18-Classification-Logistic-Regression/softmax-regression.jpg" alt="softmax-regression" style="zoom:40%;" /></p>

<ul>
  <li>
    <p>åˆ©ç”¨ Softmax Regression å¤„ç†å¤šä¸ªç±»ï¼Œè®©å¤šä¸ªç±»çš„æ¦‚ç‡å’Œç­‰äº1</p>
  </li>
  <li>
    <p>åˆ©ç”¨ Cross Entropy è®¡ç®— loss å‡½æ•°</p>
  </li>
</ul>
:ET