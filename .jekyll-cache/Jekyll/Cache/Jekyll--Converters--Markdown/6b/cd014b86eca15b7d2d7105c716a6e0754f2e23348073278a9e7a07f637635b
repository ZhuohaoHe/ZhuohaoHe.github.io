I"<h2 id="线性回归">线性回归</h2>

<h4 id="输入特征">输入特征</h4>

<ul>
  <li>
    <p>单变量, m × 1 矩阵，其中 m 为样本个数</p>

\[X_{m \times 1} = 
	\left[
 	\begin{matrix}
   		\vec x^T
  	\end{matrix}
  	\right]
, \vec x^T = (1,x_1,x_2,...,x_m)\]
  </li>
  <li>
    <p>多变量, m × (n+1) 矩阵，其中 n 为特征个数，m 为样本个数</p>

\[X_{m \times (n+1)}= 
	\left[
 	\begin{matrix}
   		{\vec x^{(1)}}^T\\
   		...\\
   		{\vec x^{(n)}}^T
  	\end{matrix}
  	\right]
, {\vec x^{(j)}}^T = (1, x_1^{(j)},x_2^{(j)},...,x_m^{(j)})\]

    <p>其中,</p>

\[x_i^{(j)} 表示第i类特征的第j个样本\]
  </li>
</ul>

<h4 id="选择线性回归模型">选择线性回归模型</h4>

<ul>
  <li>单变量，</li>
</ul>

\[h_\theta(x)=\theta_0+\theta_1x\]

<ul>
  <li>多变量，</li>
  <li>
\[h_\theta(x)^{(1)} = \theta_0+\theta_1x_1^{(1)}+...+\theta_nx_n^{(1)}\\
  ...\\
  h_\theta(x)^{(n)} = \theta_0+\theta_1x_1^{(n)}+...+\theta_nx_n^{(n)}\]
  </li>
  <li>
    <p>都可以用<strong>矩阵</strong>表示</p>

\[h_\theta(x) = X_{m \times (n+1)} \space \Theta_{(n+1) \times 1}\]

    <p>其中 Θ 矩阵为,</p>

\[\Theta_{(n+1) \times 1} =	
	\left[
 	\begin{matrix}
		\theta_0 \\
		...\\
		\theta_n
  	\end{matrix}
  	\right]\]
  </li>
</ul>

<h4 id="损失函数">损失函数</h4>

<p>损失函数用来衡量线性模型是否和所给数据是最符合的。</p>

<p>损失函数应该满足的<strong>条件</strong>：</p>

<ul>
  <li>非负：不存在负损失</li>
  <li>预测结果h(x)与真实值y的差别小，则损失小，反之损失大。即损失函数能够正确反映预测和真实结果的偏差。</li>
</ul>

<p>在线性回归中，常用的损失函数是<strong>平方</strong>损失：</p>

\[loss(h_\theta(x^{(i)}),y^{(i)})=\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]

<p>构造出合适的损失函数主要的作用是：</p>

<ul>
  <li>正确的反映出预测结果与真实值的偏差</li>
  <li>利用损失函数值不断降低为结果，寻找<strong>优化</strong>模型参数的方法</li>
</ul>

<blockquote>
  <p>目标函数：</p>

  <p>在损失函数上简单加工得到，用于接下来的优化操作</p>
</blockquote>

<p><strong>三要素</strong>：</p>

<ul>
  <li>假设:</li>
  <li>
\[h_\theta(x)=\theta_0+\theta_1x\]
  </li>
  <li>目标函数：</li>
  <li>
\[J(\theta_0,\theta_1)=\frac1{2m} \sum_{i=1}^ml(h_\theta(x^{(i)}),y^{(i)})=\frac1{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]
  </li>
  <li>优化算法：给定训练集，如何找到最优的参数 θ 使得</li>
  <li>
\[\min_{\theta_0,\theta_1}J(\theta_0,\theta_1)\]
  </li>
</ul>

<h4 id="梯度下降法寻找最优参数">梯度下降法寻找最优参数</h4>

<p><code class="language-plaintext highlighter-rouge">repeat until convergence {</code></p>

\[\theta_j:=\theta_j-\alpha\frac \partial {\partial \theta}J(\theta_0,\theta_1), for(j=0\space and \space j=1)\]

<p><code class="language-plaintext highlighter-rouge">}</code></p>

<p>利用泰勒展开证明，</p>

<p><img src="/img/in-post/Regression/proving_process.jpg" alt="proving_process" style="zoom:50%;" /></p>

<ul>
  <li>
    <p><strong>θ0和θ1要同步更新</strong></p>

    <p><img src="/img/in-post/Regression/synchronize.jpg" alt="synchronize" style="zoom:50%;" /></p>
  </li>
  <li>
    <p><strong>批处理梯度下降</strong></p>

    <p>梯度下降的每一步都使用所 有的训练样本.</p>
  </li>
</ul>

<blockquote>
  <p><strong>思考</strong></p>

  <ol>
    <li>
      <p><em>能否保证找到最优的参数？</em></p>

      <p><u>不一定，有可能进入局部最优</u></p>

      <p><img src="/img/in-post/Regression/local_optimum.jpg" alt="local optimum" style="zoom:33%;" /></p>
    </li>
    <li>
      <p>能否保证目标函数是收敛的？
  即：梯度下降法参数更新能是否保证目标函数的值下降?</p>

      <p><u>不能，有可能到达鞍点，在鞍点导数为0</u></p>

      <p><img src="/img/in-post/Regression/saddle_point.jpg" alt="saddle point" style="zoom:33%;" /></p>
    </li>
    <li>
      <p>如何选择参数α(学习率)?</p>

      <p><u>通过不断调试，找到合适的 α 值。</u></p>

      <p><u>自动收敛测试：每次迭代损失函数 J(θ) 是否减少。</u></p>

      <p><u>选择收敛条件：如定义收敛为如果 J(θ) 在一次迭代中减少不超过 10^(-3)</u></p>

      <p><img src="/img/in-post/Regression/alpha.jpg" alt="alpha" style="zoom:50%;" /></p>

      <ul>
        <li>对于足够小的 α，J(θ) 应该在每一次迭代中减小</li>
        <li>如果 α 太小，梯度下降算法会收敛很慢</li>
        <li>如果 α 太大，梯度下降算法则不会收敛：发散或震荡</li>
      </ul>

      <p><img src="/img/in-post/Regression/alpha2.jpg" alt="alpha2" style="zoom: 33%;" /></p>
    </li>
  </ol>
</blockquote>

<h4 id="特征尺度归一化">特征尺度归一化</h4>

<p>确保特征在相同的尺度，保证梯度下降在每一个方向上都是均匀的。避免出现在某一个特征已经学习完毕，而另外的特征还在缓慢的学习。</p>

<p>归一化的方法：</p>

<ul>
  <li>
    <p>范围归一化</p>

    <p>使得每个特征尽量接近某个范围，如0&lt;xi&lt;1</p>
  </li>
  <li>
    <p>零均值归一化</p>

    <p>用 xi-μ 代替 xi，其中 μ 为 x 从1到m的均值</p>
  </li>
  <li>
    <p>零均值+范围归一化</p>

    <p>前两种方法的结合</p>
  </li>
  <li>零均值单位方差归一化</li>
  <li>
\[\frac{x_i-\mu_i}{\sigma} \rightarrow x\]
  </li>
</ul>

<h4 id="正规方程">正规方程</h4>

<p><em>除了上面介绍的采用迭代的方法以外，还有哪些方法？</em></p>

<p>令目标函数的微分为 0 ，然后求解方程，得到目标函数的最低点位置。</p>

<p>用矩阵的方式表示 J(θ)</p>

\[X \Theta - y =
	\left[
 	\begin{matrix}
		(x^{(1)})^T\theta \\
		...\\
		(x^{(m)})^T\theta
  	\end{matrix}
  	\right] - 
  	\left[
 	\begin{matrix}
		y^{(1)} \\
		...\\
		y^{(m)}
  	\end{matrix}
  	\right] = 
  	 \left[
 	\begin{matrix}
		(x^{(1)})^T\theta - y^{(1)} \\
		...\\
		(x^{(m)})^T\theta - y^{(m)}
  	\end{matrix}
  	\right]\]

<p>所以，</p>

\[J(\theta)=\frac1{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac1{2m}(X\Theta-y)(X\Theta-y)^T\]

<p>解得，</p>

\[\theta = (X^TX)^{-1}X^Ty\]

<p>推导过程如下图</p>

<p><img src="/img/in-post/Regression/normal_equation.jpg" alt="normal equation" style="zoom:50%;" /></p>

<p>其中 <code class="language-plaintext highlighter-rouge">tr()</code> 为矩阵的迹</p>

<p><strong>对比 梯度下降 和 正规方程</strong></p>

<p>梯度下降：</p>

<ul>
  <li>需要选择合适的 .</li>
  <li>需要多次迭代.</li>
  <li>即使n很大，效果也很好.</li>
</ul>

<p>正规方程：</p>

<ul>
  <li>不需要选择 .</li>
  <li>不需要迭代.</li>
  <li>需要计算</li>
  <li>如果n很大，速度将会很慢.</li>
</ul>

<p><em>如果矩阵不可逆怎么办？</em></p>

<p><u>太多的特征 (如 m &lt;= n ), 需要删减一些特征, 或者进行正则化.</u></p>
:ET